---
title: 'Does Money Make A Winner?'
subtitle: "A Statistical Approach To Predicting Major League Baseball Team Success"
author: "By Aaron Ray (aaronwr2), Aaron Rogers (aaronrr2), Michael Chan (mhchan3) and Michael Johnson (mjohns44)."
date: 'December 14, 2016'
output:
  html_document:
    toc: yes
---

> "Your goal shouldn't be to buy players, your goal should be to buy wins."
>
> --- **Peter Brand** in *Moneyball*

# Introduction

*This study is presented in fulfillment of the final project requirements for STAT420: Data Analysis, Fall 2016 semester, University of Illinois at Urbana-Champaign.  Professor David Dalpiaz.*

## Project Members

- Aaron Ray (aaronwr2)
- Aaron Rogers (aaronrr2)
- Michael Chan (mhchan3)
- Michael Johnson (mjohns44)


## Project Description and Goals

Major League Baseball is the first introduction many people in the United States have to the field of statistics.  Children and adults alike recite batting averages for their favorite players, measure pitchers by their earned run average, and expect great seasons from teams loaded with high-salary players.  Everyday people have spirited debates about baseball statistics, arguing for their own theories of which statistics predict victory on the baseball field. In this study, we have put several of these theories to the test, as well as identified the key components of a robust linear regression model capable of predicting the percentage of wins in a given season. The following three topics were taken into account.

### Team Payroll

One measure that seemingly towers over all others is a team's payroll.  With the evolution of [free agency](https://en.wikipedia.org/wiki/History_of_baseball_in_the_United_States#Player_wealth_and_influence), teams compete with each other to attract the best players.  Conventional wisdom holds that the more money a team spends on player salaries, the better players it can attract, and the more games it can win.  However, recent articles from [Sports Illustrated](http://www.si.com/mlb/strike-zone/2013/10/07/mlb-playoff-teams-payroll-rays-athletics-indians-pirates) and others make the case that low-payroll teams can compete effectively with their high-payroll rivals.    

Is there a significant relationship between team payroll and team success?  

The **first goal** of this project is to evaluate the effect of team salaries on winning percentage.  

### Run Differential

[Bill James](http://www.baseball-reference.com/bullpen/Bill_James), the pioneer of a new set of baseball statistics called [sabermetrics](http://www.baseball-reference.com/bullpen/Sabermetrics), developed the [Pythagorean Theorem of Baseball](http://www.baseball-reference.com/bullpen/Pythagorean_Theorem_of_Baseball), which suggests the Run Differential is a good predictor of win percentage.

\[
Win \% = Run Differential = \frac{Runs Scored^2}{Runs Scored^2 + Runs Allowed^2} 
\]

The **second goal** of this project is to test the Pythagorean Theorem of Baseball.  

### Other Team Data

While team payroll and run differentials may provide some explanation of team success, Major League Baseball has an abundance of available data that can be analyzed for a more robust model.  

The **third goal** of this project is to use other available team-level data to find a good model for explaining and predicting team winning percentages. 

## Description of Dataset

The source of the data for this project is http://www.seanlahman.com/baseball-archive/statistics/

This dataset contains pitching, hitting, and fielding statistics for Major League Baseball from 1871 through 2015.  It includes data from the two current leagues (American and National), the four other "major" leagues (American Association, Union Association, Players League, and Federal League), and the National Association of 1871-1875. For more details, see [readme2014.txt](http://seanlahman.com/files/database/readme2014.txt)

This collection of data includes 24 tables.  For our purposes, we focused on the effect of **team salary** on regular season **winning percentage**.  We limited our dataset to two files: Teams and Salaries.  

### Input files

The Teams file has 2,805 observations of 48 variables, which is one observation per team per year.  We used 31 of the variables for this study:

- `yearID` - Year
- `G` - Games played
- `W` - Wins
- `R` - Runs scored
- `AB` - At bats
- `H` - Hits by batters
- `X2B` - Doubles
- `X3B` - Triples
- `HR` - Homeruns by batters
- `BB` - Walks by batters
- `SO` - Strikeouts by batters
- `SB` - Stolen bases
- `CS` - Caught stealing
- `HBP` - Batters hit by pitch
- `SF` - Sacrifice flies
- `RA` - Runs allowed (Opponents runs scored)
- `ER` - Earned runs allowed
- `CG` - Complete games
- `SHO` - Shutouts
- `SV` - Saves
- `IPOuts` - Outs Pitched (innings pitched x 3)
- `HA` - Hits allowed
- `HRA` - Homeruns allowed
- `BBA` - Walks allowed
- `SOA` - Strikeouts by pitchers
- `E` - Errors
- `DP` - Double Plays
- `FP` - Fielding  percentage
- `attendance` - Home attendance total
- `BPF` - Three-year park factor for batters (>100 means batter-friendly home ballpark)
- `PPF` - Three-year park factor for pitchers (>100 means pitcher-friendly home ballpark)

The Salaries file has data for all ballplayers from 1985-2015.  Joining this dataset to the Teams file adds the following field to the study:

- `salary` - Annual salary (one row per player)

Add the following computed variables:

- `totalSalary` - Team payroll (sum of `salary` for all players on a team)
- `pctSalary` - Team payroll $\div$ All payrolls
- `WinPct` - Wins $\div$ Games
- `runDifferential` - (Runs Scored^2) $\div$ (Runs Scored^2 + Runs Allowed^2)
- `runRatio` - Runs Scored $\div$ Runs Allowed

# Method

## Prepare the data

The following `R` code loads the `Teams.csv` and `Salaries.csv`. Some data manipulation is performed to compute the total salary and the percent of total salary.

```{r}
teams = read.csv("Teams.csv")
salaries = read.csv("Salaries.csv")

# Add pctSalary
salaries_by_team_year = aggregate(x=salaries$salary, by = list(salaries$teamID, salaries$yearID), FUN=sum)
colnames(salaries_by_team_year) = c("teamID", "yearID", "salary")

mlb_data = merge(teams, salaries_by_team_year, by=c("yearID", "teamID") )
mlb_data$salary = as.numeric(mlb_data$salary)

total_salary_by_year = aggregate(mlb_data$salary, by = list(mlb_data$yearID), FUN=sum)
colnames(total_salary_by_year) = c("yearID", "totalSalary")

mlb_data = merge(mlb_data, total_salary_by_year, by = c("yearID"))
mlb_data$pctSalary = mlb_data$salary / mlb_data$totalSalary

# Add winPct
mlb_data$WinPct = mlb_data$W / mlb_data$G

```

### Identify predictors that have potential prediction values in the dataset

Add Run Differential and Run Ratio

```{r}
mlb_data$runDifferential = mlb_data$R^2 / (mlb_data$R^2 + mlb_data$RA^2)
mlb_data$runRatio = mlb_data$R / mlb_data$RA
```

Remove variables that are descriptive

```{r}
mlb_lm_data = mlb_data[, setdiff(colnames(mlb_data), c("teamID", "lgID", "franchID", "divID", "teamIDBR", "teamIDlahman45", "teamIDretro", "park", "name"))]
```

Remove variables that are essentially other responses

```{r}
mlb_lm_data = mlb_lm_data[, setdiff(colnames(mlb_lm_data), c("L", "Rank", "DivWin", "WCWin", "LgWin", "WSWin"))]
```

Remove variables that are linearly dependent

```{r}
mlb_lm_data = mlb_lm_data[, setdiff(colnames(mlb_lm_data), c("salary", "totalSalary", "W", "ERA"))]
```

Remove variables that are relatively the same across all teams

```{r}
mlb_lm_data = mlb_lm_data[, setdiff(colnames(mlb_lm_data), c("G", "Ghome"))]
```

### Use VIF to investigate the correlation of variables
```{r}
library(faraway)
vif(mlb_lm_data)
```

There are a number of variables with high VIF.  Therefore, we expect the model selection process will eliminate a large number of those variables.

## Topic 1: Salary Percentage and Wins

Now that the data have been prepared, use a simple linear regression to see if `pctSalary` is a useful predictor of `WinPct` for the entire dataset. 

\[
Model_{slrpctSalary}: Y_{WinPct} = \beta_{0} + \beta_{pctSalary} X_{pctSalary} + \epsilon
\]
```{r}
pctSalary_model = lm(WinPct ~ pctSalary, data = mlb_lm_data)
```


```{r}
yearGroup = ceiling((mlb_lm_data$yearID - 1984)  / 10)
yearGroupSummary = aggregate(mlb_lm_data$yearID, by=list(yearGroup), FUN="range")
plot(WinPct ~ pctSalary, data = mlb_lm_data,
     xlab = "Team Salary Percentage",
     ylab = "Win Percentage",
     main = "Win Percentage vs Salary Percentage",
     pch  = 20,
     cex  = 1,
     col  = yearGroup)

legend(
  x = 0.07,
  y = 0.45,
  legend= c(
    "1985 to 1994",
    "1995 to 2004",
    "2005 to 2014",
    "2015"
    ),
  col = yearGroupSummary$Group.1,
  lty = 1,
  pch  = 20
)
abline(pctSalary_model, lwd = 3, col = "darkorange")
```


Record **$R^2$** values for each individual year of salary data (1985 - 2015). 

```{r}
startYear = min(mlb_lm_data$yearID)
endYear   = max(mlb_lm_data$yearID)
numYears  = endYear - startYear + 1

pctSalary_TimeAnalysis = matrix(rep(0, numYears*2), nrow = numYears)
colnames(pctSalary_TimeAnalysis) = c("Year", "R_sq")

for (i in startYear:endYear) {
  mlb_one_year = subset(mlb_lm_data, subset = yearID == i)
  pctSalary_oneyear_model = lm(WinPct ~ pctSalary, data = mlb_one_year)
  
  pctSalary_TimeAnalysis[i - startYear + 1, 1] = i
  pctSalary_TimeAnalysis[i - startYear + 1, 2] = summary(pctSalary_oneyear_model)$r.squared
}
pctSalary_TimeAnalysis=as.data.frame(pctSalary_TimeAnalysis)
```

## Topic 2: Evaluate the Pythagorean Theorem of Baseball

Use a simple linear regression to see if `runDifferential` is a useful predictor of `WinPct` for the entire dataset. Also fit a model for a simpler variable, `runRatio`, which is just Runs Scored $\div$ Runs Allowed.

```{r}
runDifferential_model = lm(WinPct ~ runDifferential, data = mlb_lm_data)
runRatio_model = lm(WinPct ~ runRatio, data = mlb_lm_data)
```

## Topic 3: Find a "good" model for predicting `WinPct`

```{r}
findBestModel = function(startYear) {
    mlb_lm_year_filtered_data = mlb_lm_data[mlb_lm_data$yearID>=startYear,]
    mlb_lm_year_filtered_data = mlb_lm_year_filtered_data[,
      setdiff(colnames(mlb_lm_year_filtered_data), c("yearID"))]
    n = nrow(mlb_lm_year_filtered_data)
    mlb_lm_both_bic = step(lm(WinPct ~ 1, data=mlb_lm_year_filtered_data), WinPct ~    (AB+H+X2B+X3B+HR+BB+SO+SB+CS+HBP+SF+ER+CG+SHO+SV+IPouts+HA+HRA+BBA+SOA+E+DP+FP+attendance+BPF+PPF+pctSalary+runDifferential) ^ 2, direction = "both", k = log(n), trace = 0) 
    mlb_lm_both_bic
  }
```

### Use Stepwise BIC to find a "good" model for year 2001 to 2015
```{r}
mlb_lm_both_bic = findBestModel(2001)
(mlb_lm_both_bic_summary = summary(mlb_lm_both_bic))

vif(mlb_lm_both_bic)
```

The Model selection process using forward BIC selected a relatively small model that rids itself of the previously shown high multicollinearity among predictors.  As expected, the `runDifferential` is a good predictor of the win percentage.  The p-val also shows that it is very significant.  The other predictors are `SV`- `Saves`, `BB` - `Walks by batters`, `CG` - `Complete games` and `pctSalary` - `The percentage of salary`.  Of the predictors, the `runDifferential` and `pctSalary` have the greatest prediction power.

### Salary Effect on winning percentage over time

The following code selects a model with the lowest BIC with end year fixed at 2015, but with start year ranging from 1985 to 2015.

```{r warning=FALSE}
# runPctSalaryTimeEffectAnalysis
maxNumYears = 2015 - 1900 + 1
pctSalaryTimeEffectAnalysis = matrix(rep(0, maxNumYears*2), nrow = maxNumYears)
colnames(pctSalaryTimeEffectAnalysis) = c("startYear", "hasPctSalary")
for (numYears in 1:maxNumYears) {
  startYear=2015-numYears + 1
  mlb_lm_both_bic = findBestModel(startYear)
  pctSalaryTimeEffectAnalysis[maxNumYears - numYears + 1, 1] = startYear
  if ("pctSalary" %in% names(mlb_lm_both_bic$coefficients)) {
    pctSalaryTimeEffectAnalysis[maxNumYears - numYears + 1, 2] = 1
  } else {
    pctSalaryTimeEffectAnalysis[maxNumYears - numYears + 1, 2] = 0
  }
}
pctSalaryTimeEffectAnalysis=as.data.frame(pctSalaryTimeEffectAnalysis)
```

The following time range (with startYear to 2015) include pctSalary in the "best" model selection:
```{r}
pctSalaryTimeEffectAnalysis$startYear[pctSalaryTimeEffectAnalysis$hasPctSalary == 1]
```

### Main model selected for further analysis

Data from 2001 to 2015

```{r}
mlb_lm_both_bic = findBestModel(2001)
mlb_lm_both_bic_summary = summary(mlb_lm_both_bic)
mlb_lm_both_bic$coefficients
```

# Results

## Topic 1: The Money Effect  

```{r}
summary(pctSalary_model)
```

The p-value shows there is a significant relationship between `pctSalary` and `WinPct`, but $R^2$ is low at only 0.128.

When fitting a model to the 31 years with salary data available, the $R^2$ varies widely.

```{r}
plot(R_sq ~ Year, data = pctSalary_TimeAnalysis, xlab = "Year", ylab = "R-Squared", main = "R-Squared values for SLR of Win % and Team Payroll %", pch  = 20, cex  = 2, col  = "blue")
abline(h = 0.128, lwd = 2, col = "red")
```


## Topic 2: Pythagorean Theorem of Baseball

The Pythagorean Theorem of Baseball states that the pct win can be predicted by runDifferential.  The quickest way to get an idea of the relationship between them is to plot it out.

```{r}
summary(runDifferential_model)
plot(WinPct~ runDifferential, data = mlb_lm_data, col = "darkgreen")
```

Judging by the plot above, there is an obviously strong relationship between `runDifferential` and `WinPct`. Let's see whether the simplest model is better than the slightly bigger model chosen by stepwise BIC

```{r}
    mlb_lm_year_filtered_data = mlb_lm_data[mlb_lm_data$yearID>=2001,]
    mlb_lm_year_filtered_data = mlb_lm_year_filtered_data[, setdiff(colnames(mlb_lm_year_filtered_data), c("yearID"))]
    lm_small = lm(WinPct ~ runDifferential, data=mlb_lm_year_filtered_data)
    
    (mlb_lm_anova = anova(lm_small, mlb_lm_both_bic))
    mlb_lm_anova_p_val = mlb_lm_anova$`Pr(>F)`[2]
```

With p-val, `r mlb_lm_anova_p_val`, we reject the Null Hypothesis of $\beta_{SV}$=$\beta_{BB}$=$\beta_{CG}$=$\beta_{pctSalary}$=0

## Topic 3: Good model for predicting Win Percentage
### Assumption Analysis
```{r echo=FALSE,warning=FALSE,message=FALSE}
library(lmtest)

plotResidual = function(lmModel, pointcol="dodgerblue", linecol="darkorange") {
  plot(fitted(lmModel),
       resid(lmModel),
       col = pointcol,
       xlab = "Fitted",
       ylab = "Residuals"
       )
  abline(h = 0, col = linecol, lwd = 2)
}

plotQQ = function(lmModel, pointcol="dodgerblue", linecol="darkorange") {
  qqnorm(resid(lmModel), main = "Normal Q-Q Plot", col = pointcol)
  qqline(resid(lmModel), col = linecol, lwd = 2)
}
summary(mlb_lm_both_bic)
```




#### Constant Variance Assumption

**Fitted versus Residuals Plot**
```{r}
plotResidual(mlb_lm_both_bic)
```

**Breusch-Pagan Test**

```{r}
(bptest_mlb_lm_both_bic = bptest(mlb_lm_both_bic))
```

From the Breusch-Pagan test, the p-val, `r bptest_mlb_lm_both_bic$p.value` , is relatively large, which means we cannot reject the null hypothesis of Homoscedasticity (constant variance.)  In addition, the residual plots show the spread of residuals are evenly distributed.  There, we conclude that the constant variance is not violated.

#### Normality Assumption

**Normal Q-Q Plot**
```{r}
plotQQ(mlb_lm_both_bic)
```

**Shapiro-Wilk Test**

```{r}
(shapiro_mlb_lm_both_bic = shapiro.test(resid(mlb_lm_both_bic)))
```

The normality assumption for this model has not been violated.  From the q-q plot, the residuals are more or less on the qqline.  From the Shapiro-Wilk Test, the p value, `r shapiro_mlb_lm_both_bic$p.value`, is large, hence, we cannot reject the null hypothesis of the data following a normal distribution.


### Other Analysis

- Number of Influential observations
```{r}
num_data = length(mlb_lm_both_bic$residuals)
num_influential = sum(cooks.distance(mlb_lm_both_bic) > 4 / length(cooks.distance(mlb_lm_both_bic)))
num_influential
```

- The portion of Influential observations is:

```{r}
num_influential / num_data
```

- Number of High leverage observations
```{r}
num_high_leverage = sum(hatvalues(mlb_lm_both_bic) > 2 * mean(hatvalues(mlb_lm_both_bic)))
num_high_leverage
```

- The portion of High leverage observations is:
```{r}
num_high_leverage / num_data
```

# Discussion

## The Money Effect

When considering the time period of 1985 to 2015 as a whole, money is positively related to Win Percentage.  The amount of money a team spends on player salaries in relation to other teams has a significant relationship to that team's Win Percentage.  However, this is not the only consderation.  Over this time period, team payroll relative to MLB payroll can explain on average only 12.8% of a team's Winning Percentage.  As we would expect, this effect fluctuates when we consider each year individually.  This "Money Effect" ranges from 0.0% to 43.5%, and is greater than 10% for 22 of the 31 years of data.  These results suggest there are better ways to predict Win Percentage.

## The Pythagorean Theorem of Baseball

The Pythagorean theorem of baseball is validated in this study. Since the run differential is statistically significant when trying to predict win percentage, it is clear that it codifies what it takes to be a winning team. Using the run differential is very useful when trying to determine the win percentage of a team (p-value < 2e-16 and a high F-statistic); however, of greater importance is that the run differential highlights two parameters of interest, runs allowed (RA) and runs scored (R). 

At its simplest, the run differential tells us teams that score more runs than they allow leads to a better win percentage. Although it may seem obvious on the face of it, digging a little deeper the run differential may tell more about how well a team is playing than from simply how many games were won. It is in this difference that highlights its power in determining how well a team is doing. If a team has a high run differential it is telling us that players can get on base and score and it also is telling us that that team can also play a good defense. This offers us a deeper analysis than simply games won.

It would be interesting for futher analysis to see how individual player statistics for batting, pitching and fielding relate to the number of runs they score/allow, i.e., what player statistics contribute to a positive run differential. This would allow scouts to tie individuals to the run differential which is useful in constructing winning teams.

## A Good Model For Predicting a Team's Win Percentage

As shown in the Results section, the stepwise BIC method produced a model for predicting the Win Percentage with just 5 predictors: `runDifferential`, `SV`, `BB`, `CG` and `pctSalary`.  
  
This model has an adjusted $R^2$ of 0.9287, so these 5 predictors can explain on average 92.9% of a team’s Winning Percentage.  This is significantly better than the 12.8% for the `pctSalary` model (which only considers a team’s payroll).  We also saw in the study Results that when we performed an Analysis of Variance, this model with 5 predictors is significantly better than the model that just uses the `runDifferential` model (the Pythagorean Theorem of Baseball).

### Interpretation of the model

To make this model meaningful, consider an average Major League Baseball team.  The team plays 162 games per season.  An average team would win as often as it loses, so it would have a .500 Win Percentage (81 wins and 81 losses).  If this team wanted to improve to a .600 win percentage (97 wins and 65 losses), that record would typically be good enough to qualify to advance to the post-season play-offs.  

Looking at the coefficients of the model:

```{r} 
coef(mlb_lm_both_bic)
```

Consider a hypothetical average team keeping the 5 predictor variables constant except for one: 

**runDifferential - Run Differential**

An increase of 1 `runDifferential` would increase the `WinPct` by 0.750.  To make this more meaningful, consider that the average runs per team per season is 682, and the average runs allowed is 682.  The Run Differential is defined as follows:

\[
Run Differential = \frac{Runs Scored^2}{Runs Scored^2 + Runs Allowed^2} = \frac{682^2}{682^2 + 682^2} = 0.500
\]

Solving this equaltion in reverse, for the hypothetical average team to increase their Win Percentage from .500 to .600, the team needs to increase the Run Differential by .133 to .633.  This means the team would need to score 896 Runs in a season, while still holding 682 Runs Allowed, and the same number of Saves, Walks, Complete Games, and Salary Percentage:

\[
Run Differential = \frac{Runs Scored^2}{Runs Scored^2 + Runs Allowed^2} = \frac{896^2}{896^2 + 682^2} = 0.633
\]

Could a team score an additional 214 runs in a season (a 31% increase) without increasing salaries, Saves, Walks or Complete Games?  This is doubtful.  This level of increase in Runs Scored would certainly mean better batters, who would command higher salaries.  These batters would also likely have more Bases on Balls (`BB` or Walks), so it would be difficult to change the Run Differential without also changing the values of the other predictor variables.

**SV - Saves**

A Save generally occurs when a relief pitcher enters the game when his team is leading by 1-3 runs and preserves the lead to the end of the game.  (For a complete definition, see http://www.baseball-reference.com/bullpen/Save).  

In this model, one extra Save per year would result in an average increase in the Win Percentage of .0027.  A team with a .500 Win Percentage could increase to .600 if it had 37.5 more Saves in a season.  In practice, this means an average team would need to add an All-Star "closer" - a pitcher who is an expert at getting saves.  Per our dataset, teams avarage 23.7 Saves per year, and the range of Saves is from 0 to 68.  An increase of 37.5 saves over the mean of 23.7 is 61.2, which is within the historical realm. 

It is theoretically possible that a team could hire one or more players to increase their Saves from 24 to 61 while holding constant the other variables (Run Differential, BB (Walks), Complete Games and Salary Percentage).  Perhaps a team could develop such pitchers through its minor league system and thereby hire them at lower than market salaries.

**BB - Bases on Balls (Walks)**


**CG - Complete Games pitched**

**pctSalary - Percentage of the Major League Baseball total player salary paid by one team**


# Appendix

## Model with Quadratic Terms

The following section investigates the effect of adding quadratic terms in the regression model.

Find a "good" model with quadratic terms

```{r,warning=FALSE,message=FALSE}
n = nrow(mlb_lm_year_filtered_data)
mlb_lm_fore_start = lm(WinPct ~ 1, data = mlb_lm_year_filtered_data)
mlb_quad_fore_bic = step(mlb_lm_fore_start, WinPct ~ AB + I(AB^2) + H + I(H^2) + X2B + I(X2B^2) + X3B + I(X3B^2) + HR + I(HR^2) + BB + I(BB^2) + SO + I(SO^2) + SB + I(SB^2) + CS + I(CS^2) + HBP + I(HBP^2) + SF + I(SF^2) + ER + I(ER^2) + CG + I(CG^2) + SHO + I(SHO^2) + SV + I(SV^2) + IPouts + I(IPouts^2) + HA + I(HA^2) + HRA + I(HRA^2) + BBA + I(BBA^2) + SOA + I(SOA^2) + E + I(E^2) + DP + I(DP^2) + FP + I(FP^2) + attendance + I(attendance^2) + BPF + I(BPF^2) + PPF + I(PPF^2) + pctSalary + I(pctSalary^2) + runDifferential + I(runDifferential^2), direction = "forward", k = log(n), trace = 0)
(mlb_quad_fore_bic_summary = summary(mlb_quad_fore_bic))
```

Recall that the model we chose in the main section is
```{r}
mlb_lm_both_bic

sqrt(mean(resid(mlb_lm_both_bic)^2))
sqrt(mean(resid(mlb_quad_fore_bic)^2))
```

It appears the quadratic model has resulted in a slightly lower RMSE. We could then use the leave-one-out cross-validated RMSE to test if the quadratic model is only better due to overfitting of the data.

Loocv RMSE Of the Quadratic Model
```{r}
get_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

(quad_loocv_rmse = get_loocv_rmse(mlb_quad_fore_bic))
```

Loocv RMSE Of main section Model
```{r}
(main_loocv_rmse = get_loocv_rmse(mlb_lm_both_bic))
```

Note that the LOOCV RMSE of the Quadratic Model is `r quad_loocv_rmse`, which is less than the main section model, `r main_loocv_rmse`.  This helps to support the notion that the Quadratic Model is better at predicting win percentage.  However, in viewing the plots of the quadratic terms below, we see several influential observations that appear to be the cause of a somewhat false quadratic relationship for the variables `BB`, `H`, `attendance`, and `IPouts`. We therefore decided to keep the main model, as it is simpler and easier to explain.

```{r }
d=mlb_lm_year_filtered_data
par(mfrow=c(2,2))
plot(WinPct ~ BB, data = d, cex = 1.5, pch = 20)
lines(d$BB, 
      predict(lm(WinPct~BB, data=d), 
      newdata = d),col = "dodgerblue", lwd = 2, lty = 1)
lines(sort(d$BB), 
      predict(lm(WinPct~BB+I(BB^2), data=d), 
      newdata = d[order(d$BB),]),col = "darkorange", lwd = 3, lty = 2)
plot(WinPct ~ attendance, data = d, cex = 1.5, pch = 20)
lines(d$attendance, 
      predict(lm(WinPct~attendance, data=d), 
      newdata = d),col = "dodgerblue", lwd = 2, lty = 1)
lines(sort(d$attendance), 
      predict(lm(WinPct~attendance+I(attendance^2), data=d), 
      newdata = d[order(d$attendance),]),col = "darkorange", lwd = 3, lty = 2)
plot(WinPct ~ H, data = d, cex = 1.5, pch = 20)
lines(d$H, 
      predict(lm(WinPct~H, data=d), 
      newdata = d),col = "dodgerblue", lwd = 2, lty = 1)
lines(sort(d$H), 
      predict(lm(WinPct~H+I(H^2), data=d), 
      newdata = d[order(d$H),]),col = "darkorange", lwd = 3, lty = 2)
plot(WinPct ~ IPouts, data = d, cex = 1.5, pch = 20)
lines(d$IPouts, 
      predict(lm(WinPct~IPouts, data=d), 
      newdata = d),col = "dodgerblue", lwd = 2, lty = 1)
lines(sort(d$IPouts), 
      predict(lm(WinPct~IPouts+I(IPouts^2), data=d[order(d$IPouts),]), 
      newdata = d[order(d$IPouts),]),col = "darkorange", lwd = 3, lty = 2)
```

## Checking Results of Final Model After Removing Influential Data Points

```{r}
final_model_wo_infl = lm(formula = WinPct ~ runDifferential + SV + BB + CG + pctSalary, data = mlb_lm_year_filtered_data, subset = cooks.distance(mlb_lm_both_bic) <=  4/length(cooks.distance(mlb_lm_both_bic)))
summary(final_model_wo_infl)$coefficients
summary(mlb_lm_both_bic)$coefficients
```
The coefficients after removing the influential points remain relatively unchanged so our initial model is still sufficient.
